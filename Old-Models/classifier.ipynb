{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to build a predictive model that is able to classify the outcome of a match in the English\n",
    "Premier League with enough accuracy to be able to make a profit in the betting market. In terms of the betting data, there is one dataset per season, each consisting of 380 rows, one for each match, and 68 columns, most of which are the odds for various bets. Of the columns that contain data that are performance metrics we choose 3 to include in our model: goals, shots and corners. We create the features by using a simple moving average to capture the short term form of a team. We do this for both of the teams competing in the match. These averages are then subtracted from each other so we end up with 3 features derived from this data. We also have avaiable to us data scraped from the web containing each team's position after each matchweek. We make use of this data by creating a 4th feature by subtracting the position of the away team from the position of the home team. This feature captures the long term performances of the teams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from ray import tune\n",
    "import torchvision.transforms as transforms\n",
    "from functools import partial\n",
    "import jellyfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_NAME = os.getcwd()\n",
    "NUM_CLASSES = 3\n",
    "NUM_FEATS = 4\n",
    "NUM_GAMES = 38\n",
    "YEAR_BEG = 13\n",
    "YEAR_END = 19\n",
    "COL_NAMES = ['goal_diff', 'shot_diff', 'corner_diff']\n",
    "NAME_RES_COLS = ['HomeTeam', 'AwayTeam', 'FTR']\n",
    "POS_COLS = ['HPOS', 'APOS']\n",
    "BET_COL_IDX = 23 \n",
    "MIN_PERIODS= 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first gather the betting data into a list of dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Div      Date     HomeTeam     AwayTeam  FTHG  FTAG FTR  HTHG  HTAG HTR  \\\n",
      "0    E0  17/08/13      Arsenal  Aston Villa     1     3   A     1     1   D   \n",
      "1    E0  17/08/13    Liverpool        Stoke     1     0   H     1     0   H   \n",
      "2    E0  17/08/13      Norwich      Everton     2     2   D     0     0   D   \n",
      "3    E0  17/08/13   Sunderland       Fulham     0     1   A     0     0   D   \n",
      "4    E0  17/08/13      Swansea   Man United     1     4   A     0     2   A   \n",
      "..   ..       ...          ...          ...   ...   ...  ..   ...   ...  ..   \n",
      "375  E0  11/05/14      Norwich      Arsenal     0     2   A     0     0   D   \n",
      "376  E0  11/05/14  Southampton   Man United     1     1   D     1     0   H   \n",
      "377  E0  11/05/14   Sunderland      Swansea     1     3   A     0     2   A   \n",
      "378  E0  11/05/14    Tottenham  Aston Villa     3     0   H     3     0   H   \n",
      "379  E0  11/05/14    West Brom        Stoke     1     2   A     0     1   A   \n",
      "\n",
      "     ... BbAv<2.5  BbAH  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA  PSCH  \\\n",
      "0    ...     2.37   NaN    NaN      NaN      NaN      NaN      NaN  1.44   \n",
      "1    ...     2.02   NaN    NaN      NaN      NaN      NaN      NaN  1.42   \n",
      "2    ...     1.82   NaN    NaN      NaN      NaN      NaN      NaN  3.81   \n",
      "3    ...     1.77   NaN    NaN      NaN      NaN      NaN      NaN  2.52   \n",
      "4    ...     1.98   NaN    NaN      NaN      NaN      NaN      NaN  3.62   \n",
      "..   ...      ...   ...    ...      ...      ...      ...      ...   ...   \n",
      "375  ...     2.36  25.0   0.75     1.85     1.80     2.12     2.07  4.97   \n",
      "376  ...     2.12  27.0   0.00     1.90     1.86     2.06     1.99  2.77   \n",
      "377  ...     1.94  24.0  -0.50     2.40     2.30     1.68     1.63  2.19   \n",
      "378  ...     2.36  29.0  -1.00     1.86     1.77     2.20     2.10  1.47   \n",
      "379  ...     1.96  25.0  -0.25     1.85     1.76     2.18     2.11  2.18   \n",
      "\n",
      "     PSCD   PSCA  \n",
      "0    5.00   8.05  \n",
      "1    4.62  10.19  \n",
      "2    3.27   2.21  \n",
      "3    3.23   3.16  \n",
      "4    3.41   2.22  \n",
      "..    ...    ...  \n",
      "375  4.29   1.70  \n",
      "376  3.92   2.47  \n",
      "377  3.60   3.50  \n",
      "378  4.75   7.64  \n",
      "379  3.69   3.44  \n",
      "\n",
      "[380 rows x 68 columns]\n",
      "    Div      Date    HomeTeam        AwayTeam  FTHG  FTAG FTR  HTHG  HTAG HTR  \\\n",
      "0    E0  16/08/14     Arsenal  Crystal Palace     2     1   H     1     1   D   \n",
      "1    E0  16/08/14   Leicester         Everton     2     2   D     1     2   A   \n",
      "2    E0  16/08/14  Man United         Swansea     1     2   A     0     1   A   \n",
      "3    E0  16/08/14         QPR            Hull     0     1   A     0     0   D   \n",
      "4    E0  16/08/14       Stoke     Aston Villa     0     1   A     0     0   D   \n",
      "..   ..       ...         ...             ...   ...   ...  ..   ...   ...  ..   \n",
      "375  E0  24/05/15        Hull      Man United     0     0   D     0     0   D   \n",
      "376  E0  24/05/15   Leicester             QPR     5     1   H     2     0   H   \n",
      "377  E0  24/05/15    Man City     Southampton     2     0   H     1     0   H   \n",
      "378  E0  24/05/15   Newcastle        West Ham     2     0   H     0     0   D   \n",
      "379  E0  24/05/15       Stoke       Liverpool     6     1   H     5     0   H   \n",
      "\n",
      "     ... BbAv<2.5  BbAH  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA  PSCH  \\\n",
      "0    ...     2.10    24  -1.50     1.81     1.78     2.20     2.10  1.29   \n",
      "1    ...     1.80    22   0.25     1.88     1.85     2.10     2.02  3.11   \n",
      "2    ...     2.13    25  -1.50     2.18     2.08     1.87     1.79  1.45   \n",
      "3    ...     1.58    24   0.00     1.80     1.73     2.25     2.14  2.31   \n",
      "4    ...     1.60    23  -0.50     1.95     1.91     2.02     1.96  2.01   \n",
      "..   ...      ...   ...    ...      ...      ...      ...      ...   ...   \n",
      "375  ...     1.99    25   0.50     1.76     1.71     2.27     2.19  3.20   \n",
      "376  ...     2.41    28  -1.00     1.98     1.93     1.98     1.93  1.53   \n",
      "377  ...     2.66    28  -1.00     2.00     1.94     2.03     1.93  1.60   \n",
      "378  ...     2.25    25  -0.50     1.82     1.78     2.20     2.10  1.76   \n",
      "379  ...     1.99    25   0.25     2.07     2.02     1.88     1.85  3.56   \n",
      "\n",
      "     PSCD   PSCA  \n",
      "0    5.90  12.75  \n",
      "1    3.40   2.47  \n",
      "2    4.81   8.25  \n",
      "3    3.24   3.59  \n",
      "4    3.34   4.51  \n",
      "..    ...    ...  \n",
      "375  3.76   2.27  \n",
      "376  4.94   6.13  \n",
      "377  4.35   6.00  \n",
      "378  4.01   4.98  \n",
      "379  3.60   2.17  \n",
      "\n",
      "[380 rows x 68 columns]\n",
      "    Div        Date     HomeTeam     AwayTeam  FTHG  FTAG FTR  HTHG  HTAG HTR  \\\n",
      "0    E0  08/08/2015  Bournemouth  Aston Villa     0     1   A     0     0   D   \n",
      "1    E0  08/08/2015      Chelsea      Swansea     2     2   D     2     1   H   \n",
      "2    E0  08/08/2015      Everton      Watford     2     2   D     0     1   A   \n",
      "3    E0  08/08/2015    Leicester   Sunderland     4     2   H     3     0   H   \n",
      "4    E0  08/08/2015   Man United    Tottenham     1     0   H     1     0   H   \n",
      "..   ..         ...          ...          ...   ...   ...  ..   ...   ...  ..   \n",
      "375  E0  15/05/2016        Stoke     West Ham     2     1   H     0     1   A   \n",
      "376  E0  15/05/2016      Swansea     Man City     1     1   D     1     1   D   \n",
      "377  E0  15/05/2016      Watford   Sunderland     2     2   D     0     1   A   \n",
      "378  E0  15/05/2016    West Brom    Liverpool     1     1   D     1     1   D   \n",
      "379  E0  17/05/2016   Man United  Bournemouth     3     1   H     1     0   H   \n",
      "\n",
      "     ... BbAv<2.5  BbAH  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA  PSCH  \\\n",
      "0    ...     1.79    26  -0.50     1.98     1.93     1.99     1.92  1.82   \n",
      "1    ...     1.99    27  -1.50     2.24     2.16     1.80     1.73  1.37   \n",
      "2    ...     1.96    26  -1.00     2.28     2.18     1.76     1.71  1.75   \n",
      "3    ...     1.67    26  -0.50     2.00     1.95     1.96     1.90  1.79   \n",
      "4    ...     2.01    26  -1.00     2.20     2.09     1.82     1.78  1.64   \n",
      "..   ...      ...   ...    ...      ...      ...      ...      ...   ...   \n",
      "375  ...     2.27    30   0.25     2.11     2.03     1.87     1.83  3.05   \n",
      "376  ...     2.45    31   1.00     2.14     2.05     1.85     1.81  7.05   \n",
      "377  ...     2.10    29  -0.50     2.06     2.01     1.91     1.85  1.64   \n",
      "378  ...     2.09    30  -0.25     2.10     2.05     1.86     1.81  2.61   \n",
      "379  ...     2.20    29  -0.75     1.88     1.84     2.08     2.02  1.50   \n",
      "\n",
      "     PSCD   PSCA  \n",
      "0    3.88   4.70  \n",
      "1    5.04  10.88  \n",
      "2    3.76   5.44  \n",
      "3    3.74   5.10  \n",
      "4    4.07   6.04  \n",
      "..    ...    ...  \n",
      "375  4.00   2.26  \n",
      "376  5.00   1.47  \n",
      "377  4.52   5.27  \n",
      "378  3.75   2.70  \n",
      "379  4.60   7.03  \n",
      "\n",
      "[380 rows x 65 columns]\n",
      "    Div      Date        HomeTeam        AwayTeam  FTHG  FTAG FTR  HTHG  HTAG  \\\n",
      "0    E0  13/08/16         Burnley         Swansea     0     1   A     0     0   \n",
      "1    E0  13/08/16  Crystal Palace       West Brom     0     1   A     0     0   \n",
      "2    E0  13/08/16         Everton       Tottenham     1     1   D     1     0   \n",
      "3    E0  13/08/16            Hull       Leicester     2     1   H     1     0   \n",
      "4    E0  13/08/16        Man City      Sunderland     2     1   H     1     0   \n",
      "..   ..       ...             ...             ...   ...   ...  ..   ...   ...   \n",
      "375  E0  21/05/17       Liverpool   Middlesbrough     3     0   H     1     0   \n",
      "376  E0  21/05/17      Man United  Crystal Palace     2     0   H     2     0   \n",
      "377  E0  21/05/17     Southampton           Stoke     0     1   A     0     0   \n",
      "378  E0  21/05/17         Swansea       West Brom     2     1   H     0     1   \n",
      "379  E0  21/05/17         Watford        Man City     0     5   A     0     4   \n",
      "\n",
      "    HTR  ... BbAv<2.5  BbAH  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA   PSCH  \\\n",
      "0     D  ...     1.61    32  -0.25     2.13     2.06     1.86     1.81   2.79   \n",
      "1     D  ...     1.52    33  -0.50     2.07     2.00     1.90     1.85   2.25   \n",
      "2     H  ...     1.77    32   0.25     1.91     1.85     2.09     2.00   3.64   \n",
      "3     H  ...     1.67    31   0.25     2.35     2.26     2.03     1.67   4.68   \n",
      "4     H  ...     2.48    34  -1.50     1.81     1.73     2.20     2.14   1.25   \n",
      "..   ..  ...      ...   ...    ...      ...      ...      ...      ...    ...   \n",
      "375   H  ...     2.73    18  -2.50     2.02     1.97     1.95     1.90   1.15   \n",
      "376   H  ...     1.81    19  -0.25     2.19     2.11     1.85     1.79   2.35   \n",
      "377   D  ...     2.01    18  -0.75     2.03     1.98     1.93     1.88   1.64   \n",
      "378   A  ...     1.98    19  -0.50     2.11     2.06     1.86     1.82   2.29   \n",
      "379   A  ...     2.80    18   1.75     1.99     1.94     2.00     1.93  18.00   \n",
      "\n",
      "      PSCD   PSCA  \n",
      "0     3.16   2.89  \n",
      "1     3.15   3.86  \n",
      "2     3.54   2.16  \n",
      "3     3.50   1.92  \n",
      "4     6.50  14.50  \n",
      "..     ...    ...  \n",
      "375  10.50  19.09  \n",
      "376   3.38   3.35  \n",
      "377   4.37   5.53  \n",
      "378   3.45   3.40  \n",
      "379   9.70   1.16  \n",
      "\n",
      "[380 rows x 65 columns]\n",
      "    Div        Date        HomeTeam      AwayTeam  FTHG  FTAG FTR  HTHG  HTAG  \\\n",
      "0    E0  11/08/2017         Arsenal     Leicester     4     3   H     2     2   \n",
      "1    E0  12/08/2017        Brighton      Man City     0     2   A     0     0   \n",
      "2    E0  12/08/2017         Chelsea       Burnley     2     3   A     0     3   \n",
      "3    E0  12/08/2017  Crystal Palace  Huddersfield     0     3   A     0     2   \n",
      "4    E0  12/08/2017         Everton         Stoke     1     0   H     1     0   \n",
      "..   ..         ...             ...           ...   ...   ...  ..   ...   ...   \n",
      "375  E0  13/05/2018       Newcastle       Chelsea     3     0   H     1     0   \n",
      "376  E0  13/05/2018     Southampton      Man City     0     1   A     0     0   \n",
      "377  E0  13/05/2018         Swansea         Stoke     1     2   A     1     2   \n",
      "378  E0  13/05/2018       Tottenham     Leicester     5     4   H     1     2   \n",
      "379  E0  13/05/2018        West Ham       Everton     3     1   H     1     0   \n",
      "\n",
      "    HTR  ... BbAv<2.5  BbAH  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA   PSCH  \\\n",
      "0     D  ...     2.32    21  -1.00     1.91     1.85     2.10     2.02   1.49   \n",
      "1     D  ...     2.27    20   1.50     1.95     1.91     2.01     1.96  11.75   \n",
      "2     A  ...     2.23    20  -1.75     2.03     1.97     1.95     1.90   1.33   \n",
      "3     A  ...     1.72    18  -0.75     2.10     2.05     1.86     1.83   1.79   \n",
      "4     H  ...     1.76    19  -0.75     1.94     1.90     2.01     1.98   1.82   \n",
      "..   ..  ...      ...   ...    ...      ...      ...      ...      ...    ...   \n",
      "375   H  ...     2.01    21   1.00     1.90     1.83     2.11     2.03   4.85   \n",
      "376   D  ...     2.49    20   1.25     2.01     1.95     1.97     1.91   6.32   \n",
      "377   A  ...     2.06    19  -0.50     1.94     1.88     2.03     1.98   2.08   \n",
      "378   A  ...     2.84    20  -1.50     1.96     1.86     2.05     2.00   1.38   \n",
      "379   H  ...     1.97    21  -0.25     2.09     2.03     1.88     1.84   2.35   \n",
      "\n",
      "     PSCD   PSCA  \n",
      "0    4.73   7.25  \n",
      "1    6.15   1.29  \n",
      "2    5.40  12.25  \n",
      "3    3.56   5.51  \n",
      "4    3.49   5.42  \n",
      "..    ...    ...  \n",
      "375  3.72   1.80  \n",
      "376  4.78   1.51  \n",
      "377  3.56   3.82  \n",
      "378  5.50   8.15  \n",
      "379  3.40   3.28  \n",
      "\n",
      "[380 rows x 65 columns]\n",
      "    Div        Date      HomeTeam        AwayTeam  FTHG  FTAG FTR  HTHG  HTAG  \\\n",
      "0    E0  10/08/2018    Man United       Leicester     2     1   H     1     0   \n",
      "1    E0  11/08/2018   Bournemouth         Cardiff     2     0   H     1     0   \n",
      "2    E0  11/08/2018        Fulham  Crystal Palace     0     2   A     0     1   \n",
      "3    E0  11/08/2018  Huddersfield         Chelsea     0     3   A     0     2   \n",
      "4    E0  11/08/2018     Newcastle       Tottenham     1     2   A     1     2   \n",
      "..   ..         ...           ...             ...   ...   ...  ..   ...   ...   \n",
      "375  E0  12/05/2019     Liverpool          Wolves     2     0   H     1     0   \n",
      "376  E0  12/05/2019    Man United         Cardiff     0     2   A     0     1   \n",
      "377  E0  12/05/2019   Southampton    Huddersfield     1     1   D     1     0   \n",
      "378  E0  12/05/2019     Tottenham         Everton     2     2   D     1     0   \n",
      "379  E0  12/05/2019       Watford        West Ham     1     4   A     0     2   \n",
      "\n",
      "    HTR  ... BbAv<2.5  BbAH  BbAHh  BbMxAHH  BbAvAHH  BbMxAHA  BbAvAHA  PSCH  \\\n",
      "0     H  ...     1.79    17  -0.75     1.75     1.70     2.29     2.21  1.55   \n",
      "1     H  ...     1.83    20  -0.75     2.20     2.13     1.80     1.75  1.88   \n",
      "2     A  ...     1.87    22  -0.25     2.18     2.11     1.81     1.77  2.62   \n",
      "3     A  ...     1.84    23   1.00     1.84     1.80     2.13     2.06  7.24   \n",
      "4     A  ...     1.81    20   0.25     2.20     2.12     1.80     1.76  4.74   \n",
      "..   ..  ...      ...   ...    ...      ...      ...      ...      ...   ...   \n",
      "375   H  ...     2.31    22  -1.50     1.98     1.91     2.01     1.95  1.32   \n",
      "376   A  ...     2.95    21  -2.00     2.52     2.32     1.72     1.64  1.30   \n",
      "377   H  ...     2.29    22  -1.50     2.27     2.16     1.80     1.73  1.37   \n",
      "378   H  ...     2.07    19  -0.50     2.13     2.08     1.85     1.80  1.91   \n",
      "379   A  ...     2.44    19  -0.50     2.25     2.19     1.78     1.72  2.11   \n",
      "\n",
      "     PSCD  PSCA  \n",
      "0    4.07  7.69  \n",
      "1    3.61  4.70  \n",
      "2    3.38  2.90  \n",
      "3    3.95  1.58  \n",
      "4    3.53  1.89  \n",
      "..    ...   ...  \n",
      "375  5.89  9.48  \n",
      "376  6.06  9.71  \n",
      "377  5.36  8.49  \n",
      "378  3.81  4.15  \n",
      "379  3.86  3.41  \n",
      "\n",
      "[380 rows x 62 columns]\n"
     ]
    }
   ],
   "source": [
    "def read_data(beg, end):\n",
    "    '''\n",
    "    Stores data for all seasons between beg and end in separate \n",
    "    DataFrames.\n",
    "    \n",
    "    Parameters:\n",
    "    beg (int): int of the season to start at (e.g. 12)\n",
    "    end (int): int of the season to end at\n",
    "    \n",
    "    Returns:\n",
    "    datasets (list): list of DataFrames each containing the data for a \n",
    "    particular season\n",
    "    '''\n",
    "    years = [str(i) + str(i+1) for i in range(beg,end)]\n",
    "    datasets = []\n",
    "    for suffix in years:\n",
    "        csv = DIR_NAME + '/data/match_stats' + suffix + '.csv' \n",
    "        dat = pd.read_csv(csv)\n",
    "        datasets.append(dat)  \n",
    "    return datasets\n",
    "\n",
    "datasets = read_data(YEAR_BEG, YEAR_END)\n",
    "for data in datasets:\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the net score as opposed to the raw score for each metric so we do not penalize more defensive minded teams who may have a relatively low raw score but also restrict their opponent's scores. We also print the proportions of each class to get a sense of the difficulty of the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20526315789473684 0.3236842105263158 0.4710526315789474\n",
      "0.24473684210526317 0.3026315789473684 0.45263157894736844\n",
      "0.28157894736842104 0.30526315789473685 0.4131578947368421\n",
      "0.22105263157894736 0.2868421052631579 0.4921052631578947\n",
      "0.26052631578947366 0.28421052631578947 0.45526315789473687\n",
      "0.1868421052631579 0.3368421052631579 0.4763157894736842\n"
     ]
    }
   ],
   "source": [
    "for data in datasets:\n",
    "    data['goal_diff'] = data['FTHG'] - data['FTAG']\n",
    "    data['shot_diff'] = data['HST'] - data['AST']\n",
    "    data['corner_diff'] = data['HC'] - data['AC']\n",
    "    size = data.shape[0]\n",
    "    \n",
    "    print(data[data['FTR'] == 'D'].shape[0]/size, data[\n",
    "          data['FTR'] == 'A'].shape[0]/size,      \n",
    "          data[data['FTR'] == 'H'].shape[0]/size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we read the positional data into a list of lists, with the (i,j) <sup> th </sup> entry being the league table during season i after MIN_PERIOD + j games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pos_data(beg, end):\n",
    "    '''\n",
    "    Reads the positional data into a list of lists. The outer level is \n",
    "    the season, and the inner level is the round.\n",
    "    \n",
    "    Paramaters:\n",
    "    beg (int): int of the season to start at (e.g. 12)\n",
    "    end (int): int of the season to end at\n",
    "    \n",
    "    Returns:\n",
    "    dfs (list of list): a list of lists of DataFrames each containing the \n",
    "    positional data for a given season and round\n",
    "    \n",
    "    '''\n",
    "    dfs = []\n",
    "    for yr in [str(i) + '-' + str(i+1) for i in range(2000+beg,2000+end)]:\n",
    "        season_dfs = []\n",
    "        for rd in range(MIN_PERIODS, NUM_GAMES):\n",
    "            fpath = DIR_NAME + '/data/table-{}-{}.csv'.format(yr, rd)\n",
    "            season_dfs.append(pd.read_csv(fpath, index_col=0, names=[\n",
    "                'Team', 'Match'], header=0))\n",
    "        dfs.append(season_dfs)\n",
    "    return dfs\n",
    "\n",
    "pos_dfs = read_pos_data(YEAR_BEG, YEAR_END)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must resolve the inconsistencies in the names of teams between the two data sources. Rather than hard code the correct mapping for each season, we use the jaro winkler similarity score to automate the process and check for any errors. In the end, only Man City was incorrectly classified so we correct it by hand.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Arsenal': 'Arsenal FC', 'Liverpool': 'Liverpool FC', 'Norwich': 'Norwich City', 'Sunderland': 'Sunderland AFC', 'Swansea': 'Swansea City', 'West Brom': 'West Bromwich Albion', 'West Ham': 'West Ham United', 'Chelsea': 'Chelsea FC', 'Crystal Palace': 'Crystal Palace', 'Man City': 'Manchester City', 'Aston Villa': 'Aston Villa', 'Everton': 'Everton FC', 'Fulham': 'Fulham FC', 'Hull': 'Hull City', 'Newcastle': 'Newcastle United', 'Southampton': 'Southampton FC', 'Stoke': 'Stoke City', 'Cardiff': 'Cardiff City', 'Tottenham': 'Tottenham Hotspur', 'Man United': 'Manchester United'}\n",
      "{'Arsenal': 'Arsenal FC', 'Leicester': 'Leicester City', 'Man United': 'Manchester United', 'QPR': 'Queens Park Rangers', 'Stoke': 'Stoke City', 'West Brom': 'West Bromwich Albion', 'West Ham': 'West Ham United', 'Liverpool': 'Liverpool FC', 'Newcastle': 'Newcastle United', 'Burnley': 'Burnley FC', 'Aston Villa': 'Aston Villa', 'Chelsea': 'Chelsea FC', 'Crystal Palace': 'Crystal Palace', 'Everton': 'Everton FC', 'Southampton': 'Southampton FC', 'Swansea': 'Swansea City', 'Hull': 'Hull City', 'Sunderland': 'Sunderland AFC', 'Tottenham': 'Tottenham Hotspur', 'Man City': 'Manchester City'}\n",
      "{'Bournemouth': 'AFC Bournemouth', 'Chelsea': 'Chelsea FC', 'Everton': 'Everton FC', 'Leicester': 'Leicester City', 'Man United': 'Manchester United', 'Norwich': 'Norwich City', 'Arsenal': 'Arsenal FC', 'Newcastle': 'Newcastle United', 'Stoke': 'Stoke City', 'West Brom': 'West Bromwich Albion', 'Aston Villa': 'Aston Villa', 'Southampton': 'Southampton FC', 'Sunderland': 'Sunderland AFC', 'Swansea': 'Swansea City', 'Tottenham': 'Tottenham Hotspur', 'Watford': 'Watford FC', 'West Ham': 'West Ham United', 'Crystal Palace': 'Crystal Palace', 'Man City': 'Manchester City', 'Liverpool': 'Liverpool FC'}\n",
      "{'Burnley': 'Burnley FC', 'Crystal Palace': 'Crystal Palace', 'Everton': 'Everton FC', 'Hull': 'Hull City', 'Man City': 'Manchester City', 'Middlesbrough': 'Middlesbrough FC', 'Southampton': 'Southampton FC', 'Arsenal': 'Arsenal FC', 'Bournemouth': 'AFC Bournemouth', 'Chelsea': 'Chelsea FC', 'Man United': 'Manchester United', 'Leicester': 'Leicester City', 'Stoke': 'Stoke City', 'Swansea': 'Swansea City', 'Tottenham': 'Tottenham Hotspur', 'Watford': 'Watford FC', 'West Brom': 'West Bromwich Albion', 'Sunderland': 'Sunderland AFC', 'West Ham': 'West Ham United', 'Liverpool': 'Liverpool FC'}\n",
      "{'Arsenal': 'Arsenal FC', 'Brighton': 'Brighton & Hove Albion', 'Chelsea': 'Chelsea FC', 'Crystal Palace': 'Crystal Palace', 'Everton': 'Everton FC', 'Southampton': 'Southampton FC', 'Watford': 'Watford FC', 'West Brom': 'West Bromwich Albion', 'Man United': 'Manchester United', 'Newcastle': 'Newcastle United', 'Bournemouth': 'AFC Bournemouth', 'Burnley': 'Burnley FC', 'Leicester': 'Leicester City', 'Liverpool': 'Liverpool FC', 'Stoke': 'Stoke City', 'Swansea': 'Swansea City', 'Huddersfield': 'Huddersfield Town', 'Tottenham': 'Tottenham Hotspur', 'Man City': 'Manchester City', 'West Ham': 'West Ham United'}\n",
      "{'Man United': 'Manchester United', 'Bournemouth': 'AFC Bournemouth', 'Fulham': 'Fulham FC', 'Huddersfield': 'Huddersfield Town', 'Newcastle': 'Newcastle United', 'Watford': 'Watford FC', 'Wolves': 'Wolverhampton Wanderers', 'Arsenal': 'Arsenal FC', 'Liverpool': 'Liverpool FC', 'Southampton': 'Southampton FC', 'Cardiff': 'Cardiff City', 'Chelsea': 'Chelsea FC', 'Everton': 'Everton FC', 'Leicester': 'Leicester City', 'Tottenham': 'Tottenham Hotspur', 'West Ham': 'West Ham United', 'Brighton': 'Brighton & Hove Albion', 'Burnley': 'Burnley FC', 'Man City': 'Manchester City', 'Crystal Palace': 'Crystal Palace'}\n"
     ]
    }
   ],
   "source": [
    "def get_mapping(arr1, arr2):\n",
    "    '''\n",
    "    Establishes a correspondence between two arrays of strings based \n",
    "    on the jaro winkler similarity score. Each string in the first array is \n",
    "    matched to the string in the second array with the highest similarity score.\n",
    "    \n",
    "    Parameters:\n",
    "    arr1 (iterable): an array of strings\n",
    "    arr2 (iterable): an array of strings\n",
    "    \n",
    "    Returns:\n",
    "    mapping (dic): map that associates a string in one array with the most \n",
    "    similar string in the other array\n",
    "    \n",
    "    note: the map is not necessarily 1-1 or surjective\n",
    "    '''\n",
    "    mapping = {}\n",
    "    for name1 in arr1:\n",
    "        max_score = np.NINF\n",
    "        closest = ''\n",
    "        for name2 in arr2:\n",
    "            score = jellyfish.jaro_winkler_similarity(name1, name2)\n",
    "            if score > max_score:\n",
    "                closest = name2\n",
    "                max_score = score\n",
    "        mapping[name1] = closest\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "maps = [get_mapping(pd.unique(data['HomeTeam']), \n",
    "                    pd.unique(pos_dfs[i][0]['Team'])\n",
    "                   ) for i, data in enumerate(datasets)]\n",
    "for i in range(len(maps)):\n",
    "    maps[i]['Man City'] = 'Manchester City'\n",
    "    print(maps[i])\n",
    "datasets = [data.replace(maps[i]) for i, data in enumerate(datasets)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to match the datasets together we need to know which round it is for each match in the betting dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_index(row, dic):\n",
    "    '''\n",
    "    Gets the round of a given match. We subtract MIN_PERIODS so that \n",
    "    the index of the round starts at MIN_PERIODS\n",
    "    \n",
    "    Parameters:\n",
    "    row (indexable): row containing the home team and away team.\n",
    "    dic: dictionary mapping the team to the current number of rounds it has played\n",
    "    \n",
    "    Returns: tuple containing the number of rounds played by the home and away \n",
    "    teams, respectively\n",
    "    '''\n",
    "    dic[row[0]] += 1\n",
    "    dic[row[1]] += 1\n",
    "    return dic[row[0]] - MIN_PERIODS - 1, dic[row[1]] - MIN_PERIODS - 1\n",
    "\n",
    "def create_pos_index_cols(df):\n",
    "    '''\n",
    "    Creates dataframe containing two columns: the number of games played by the \n",
    "    home team and the games played by the away team, not including the current \n",
    "    match. As usual, the index is the match number.\n",
    "    \n",
    "    Parameters:\n",
    "    df: dataframe containing the names of the home and away teams\n",
    "    Returns: \n",
    "    df: dataframe containing the number of games played by the home and away teams\n",
    "    '''\n",
    "    dic = {}\n",
    "    for name in pd.unique(df['HomeTeam']):\n",
    "            dic[name] = 0\n",
    "    games_df = df[['HomeTeam', 'AwayTeam']].apply(\n",
    "        get_pos_index, axis=1, args=(dic,), result_type='expand').rename(\n",
    "        {0:'HINDEX', 1:'AINDEX'},axis=1)\n",
    "    df = df.join(games_df)\n",
    "    return df\n",
    "\n",
    "datasets = [create_pos_index_cols(data) for data in datasets]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the round numbers for every match, we know which dataframe to look up in order to get both teams' positions at the start of the match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(row, pos_dfs_lst):\n",
    "    '''\n",
    "    Gets the position of both teams prior to the start of the match.\n",
    "    Parameters:\n",
    "    row (indexable): row containing the Home Team, Away Team, matches \n",
    "    played by the home team, matches played by the away team\n",
    "    pos_dfs_lst (list): a list of lists of DataFrames each containing the \n",
    "    positional data for a given season and round \n",
    "    '''\n",
    "    hpos = pos_dfs_lst[row[2]].loc[pos_dfs_lst[row[2]]['Team']==row[0]].index.values[0]\n",
    "    apos = pos_dfs_lst[row[3]].loc[pos_dfs_lst[row[3]]['Team']==row[1]].index.values[0]\n",
    "    return hpos, apos\n",
    "\n",
    "def create_pos_cols(df, pos_df):\n",
    "    '''\n",
    "    Creates DataFrame containing the postional information of both team prior to\n",
    "    the start of the match\n",
    "    Parameters:\n",
    "    df: dataframe containing the match data\n",
    "    pos_df: dataframe containing the positional data\n",
    "    Returns: DataFrame containing the postional information of both team prior to \n",
    "    the start of the match\n",
    "    '''\n",
    "    pos_cols_df = df.loc[MIN_PERIODS*10:,(\n",
    "        'HomeTeam', 'AwayTeam', 'HINDEX','AINDEX')].apply(\n",
    "        get_pos, args=(pos_df,), axis=1, result_type='expand'\n",
    "                               ).rename({0:'HPOS', 1:'APOS'},axis=1)\n",
    "    df = df.join(pos_cols_df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "datasets = [create_pos_cols(data, pos_dfs[i]) for i, data in enumerate(datasets)]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then collect the relevant performance metrics (goal, shot and corner differences) for a single team into one data frame. We also note whether that team was the away team for that game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_team_df(df, name, col_names=COL_NAMES):\n",
    "    '''\n",
    "    Gets the columns in COL_NAMES for a given team. Also notes whether or not \n",
    "    that team was the away team for that game.\n",
    "    \n",
    "    Paramaters:\n",
    "    df (DataFrame): the data for a given season\n",
    "    name (str): the name of the team\n",
    "    \n",
    "    Returns:\n",
    "    df (DataFrame): DataFrame where each row is a match played by a \n",
    "    given team and the columns are those in COL_NAMES and a boolean column \n",
    "    indicating whether or not the team was the away team.\n",
    "    \n",
    "    '''\n",
    "    home_df = df.loc[df['HomeTeam']==name][col_names].assign(is_away=False)\n",
    "    away_df = (-1*df.loc[df['AwayTeam']==name][col_names]).assign(is_away=True)\n",
    "    df = pd.concat((home_df, away_df)).sort_index()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a summary of a team's short term form by taking a simple average of these differences over the past MIN_PERIOD games. We use this summary to create the feature matrix by first collecting all the averages for a given team during a given season. Once we've gathered all the averages for a single season into one DataFrame we combined the averages with the same index by subtracting the away team's average from the home team's average. We also append a column cotaining the difference between the home and away teams' postions, so we end up with 4 features in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(330, 3)\n",
      "(330, 3)\n",
      "(330, 3)\n",
      "(330, 3)\n",
      "(330, 3)\n",
      "(330, 3)\n",
      "(1980,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>goal_diff</th>\n",
       "      <th>shot_diff</th>\n",
       "      <th>corner_diff</th>\n",
       "      <th>pos_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.6</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>-3.2</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-2.6</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.8</td>\n",
       "      <td>-12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>-0.6</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>-4.6</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1980 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      goal_diff  shot_diff  corner_diff  pos_diff\n",
       "0          -1.6       -3.4         -3.2      10.0\n",
       "1          -0.4       -0.2         -2.6       2.0\n",
       "2          -0.4        0.8          0.0      -4.0\n",
       "3           0.2        1.6          5.0      -6.0\n",
       "4           1.0        3.2          6.8     -12.0\n",
       "...         ...        ...          ...       ...\n",
       "1975        2.0        1.0         -2.8      -5.0\n",
       "1976       -0.2       -0.8          0.0     -12.0\n",
       "1977        2.0        4.0          3.4      -4.0\n",
       "1978       -0.6       -2.2         -4.6      -4.0\n",
       "1979       -1.0       -2.0         -0.6      -1.0\n",
       "\n",
       "[1980 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_feats_and_targets(datasets):\n",
    "    '''\n",
    "    Creates features consisting of the estimate of the home team's short term \n",
    "    performance in 3 metrics (goals, shots,corners) subtracted by the estimate \n",
    "    of the away team's short term performance in those metrics. Also contains a \n",
    "    column consisting of the difference in the Home and Away team's position in \n",
    "    the league table, prior to the startof the match. Also creates the a vector \n",
    "    of targets which are the outcomes of each match and a list of indices into the \n",
    "    seasonal dataframes, which are used to match the odds for a match with the \n",
    "    probabilities outputted by our model.\n",
    "    \n",
    "    Parameters:\n",
    "    datasets (list): list of DataFrames, one for each season\n",
    "    Returns:\n",
    "    feat_mat (DataFrame): feature matrix as described above\n",
    "    y (Series): series of outcomes for each match \n",
    "    indices (list): list of indices into the seasonal dataframes\n",
    "    '''\n",
    "    feat_mat = pd.DataFrame()\n",
    "    y = pd.Series(dtype='int64')\n",
    "    indices = []\n",
    "    for data in datasets:\n",
    "        ma_df = pd.DataFrame(columns = COL_NAMES)\n",
    "        for name in pd.unique(data['HomeTeam']):\n",
    "            df = get_team_df(data, name)\n",
    "            df = df[COL_NAMES].rolling(\n",
    "                window=MIN_PERIODS).mean().shift().dropna().assign(\n",
    "                is_away=df['is_away'])\n",
    "            df = df.mask(df['is_away'], -df)[COL_NAMES]\n",
    "            ma_df = pd.concat((ma_df, df)).sort_index()\n",
    "        index = ma_df.index.unique()\n",
    "        y = pd.concat((y, data['FTR'].loc[index]))\n",
    "        feats = ma_df.groupby(ma_df.index).sum()\n",
    "        feats['pos_diff'] = data.loc[index]['HPOS'] - data.loc[index]['APOS']\n",
    "        feat_mat = pd.concat((feat_mat, feats))\n",
    "        indices.append(index)\n",
    "        print(ma_df.groupby(ma_df.index).sum().shape)\n",
    "    feat_mat.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    y = pd.Series(label_encoder.fit_transform(y))\n",
    "    print(y.shape)\n",
    "    return feat_mat, y, indices\n",
    "\n",
    "feat_mat, y, indices = create_feats_and_targets(datasets)\n",
    "feat_mat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In partitioning the data, we set aside the most recent season to use as the test set and split the data from the remaining \n",
    "seasons into the training and validition sets, with an 80-20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_start_idx = (NUM_GAMES - MIN_PERIODS)*10*(len(datasets)-1) \n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    feat_mat.iloc[:test_start_idx,:], y[:test_start_idx],\n",
    "                                            test_size=.2, random_state=42)\n",
    "X_tst = feat_mat.iloc[test_start_idx:,:]\n",
    "y_tst = y[test_start_idx:]\n",
    "y_tst.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't use sklearn's GridSearchCV to tune the hyperparameters of our model as that would prevent us from setting parameters in XGBClassifier's fit method, though this doesn't appear to be the case anymore as of Xgboost v 1.6.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparams(mod, Xtr, ytr, Xval, yval, param_grid, fit_param_dic):\n",
    "    '''\n",
    "    Fits model for each element in the cartesian product of the lists \n",
    "    of possible values of each parameter given by the param_grid. Then selects \n",
    "    the best model based on the validation accuracy.\n",
    "    \n",
    "    Parameters:\n",
    "    mod (sklearn obj): A multiclass classifier\n",
    "    Xtr (np.array): training data\n",
    "    ytr (np.array): 1D array of training labels\n",
    "    Xval (np.array): validation data\n",
    "    yval (np.array): 1D array of validation labels\n",
    "    param_grid (dic): maps each parameter to a list of values to be tested\n",
    "    fit_param_dic (dic): used to implement early stopping for xgboost. Is an \n",
    "    empty dictionary for all other classifiers\n",
    "    Returns:\n",
    "    validation_scores (dic): maps a given combination of parameters to the \n",
    "    validation score for that model\n",
    "    best_params_dic (dic): maps the parameter to its optimals value\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    validation_scores = {}\n",
    "    keys, vals = zip(*sorted(param_grid.items()))\n",
    "    for it, params in enumerate(itertools.product(*vals)):\n",
    "        #print(dict(zip(keys, params)))\n",
    "        if it % 50 == 0:\n",
    "            print(it, params)\n",
    "        mod.set_params(**dict(zip(keys, params)))\n",
    "        pipe = Pipeline(steps=[('scaler', preprocessing.StandardScaler()),\n",
    "                               ('clf', mod)])\n",
    "        pipe.fit(Xtr, ytr, **fit_param_dic)\n",
    "        validation_scores[params] = log_loss(y_val, pipe.predict_proba(X_val),\n",
    "                                             labels=y_val)\n",
    "    best_params = min(validation_scores, key=validation_scores.get)\n",
    "    best_params_dic = dict(zip(keys, best_params))\n",
    "    \n",
    "    return validation_scores, best_params_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performances of the models are similar enough that we would prefer the multinomial logistic regression model even if it didn't have the lowest log loss, due to its simplicity and interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (1, 0.1, 5, 0, 1, 9, 9, 500, 3, 'multi:softmax', 0.95)\n",
      "{'colsample_bytree': 1, 'eta': 0.1, 'gamma': 5, 'lambda': 0, 'max_delta_step': 1, 'max_depth': 9, 'min_child_weight': 9, 'n_estimators': 500, 'num_class': 3, 'objective': 'multi:softmax', 'subsample': 0.95}\n",
      "0 (1, 0.001, 'rbf', 1)\n",
      "{'C': 1, 'gamma': 0.001, 'kernel': 'rbf', 'probability': 1}\n",
      "0 (0.001, 'multinomial', 'l1', 'saga')\n",
      "50 (1.072267222010323, 'multinomial', 'l1', 'saga')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 23.10129700083158, 'multi_class': 'multinomial', 'penalty': 'l1', 'solver': 'saga'}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:50:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5878787878787879 0.9559858627391584 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, eta=0.1, gamma=5,\n",
      "              gpu_id=-1, importance_type='gain', interaction_constraints='',\n",
      "              lambda=0, learning_rate=0.100000001, max_delta_step=1,\n",
      "              max_depth=9, min_child_weight=9, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=500, n_jobs=4,\n",
      "              num_class=3, num_parallel_tree=1, objective='multi:softprob',\n",
      "              random_state=0, reg_alpha=0, reg_lambda=0, scale_pos_weight=None,\n",
      "              subsample=0.95, tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, ...)\n",
      "0.5575757575757576 0.9588924812463246 SVC(C=1, gamma=0.001, probability=1)\n",
      "0.5636363636363636 0.9366139438387908 LogisticRegression(C=23.10129700083158, multi_class='multinomial', penalty='l1',\n",
      "                   solver='saga')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('softmax',\n",
       "                 LogisticRegression(C=23.10129700083158,\n",
       "                                    multi_class='multinomial', penalty='l1',\n",
       "                                    solver='saga'))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_select_sklearn(param_grid, mods, X_tr, y_tr, X_val, y_val):\n",
    "    '''\n",
    "    For each model class under consideration, the hyperparameters are tuned to \n",
    "    find the best model within the class. The best models of each class are refit\n",
    "    on the training and validation sets and the test error is computed and the model\n",
    "    with the highest test accuracy is chosen.\n",
    "    \n",
    "    Parameters:\n",
    "    param_grid (dic): maps each parameter to a list of values to be tested\n",
    "    mods (lst): list of multiclass classifiers\n",
    "    Xtr (np.array): training data\n",
    "    ytr (np.array): 1D array of training labels\n",
    "    Xval (np.array): validation data\n",
    "    yval (np.array): 1D array of validation labels\n",
    "    \n",
    "    Returns:\n",
    "    best_mod (obj): the model with the highest test accuracy\n",
    "    '''\n",
    "    optimal_mods = []\n",
    "    for name, mod in mods:\n",
    "        fit_params = {}\n",
    "        if name == 'xgb':\n",
    "            fit_params = {'clf__early_stopping_rounds': 50,\n",
    "                          'clf__eval_set': [(X_val.values, y_val.values)], \n",
    "                          'clf__eval_metric':['merror','mlogloss'],\n",
    "                          'clf__verbose':False}\n",
    "        val_scores, best_params = tune_hyperparams(\n",
    "            mod, X_tr, y_tr, X_val, y_val, param_grids[name], fit_params)\n",
    "        print(best_params)\n",
    "        pipe = Pipeline(steps=[('scaler', preprocessing.StandardScaler()),\n",
    "                               (name, mod.set_params(**best_params))])\n",
    "        optimal_mods.append(pipe)\n",
    "    plot_xgb(optimal_mods[0]['xgb'])\n",
    "    top_logloss = np.inf\n",
    "    for mod in optimal_mods:\n",
    "        mod.fit(pd.concat((X_tr, X_val)), pd.concat((y_tr, y_val)))  #refit best models on training and validation sets\n",
    "        test_acc = mod.score(X_tst, y_tst)\n",
    "        test_logloss = log_loss(y_tst, mod.predict_proba(X_tst), labels=y_tst)\n",
    "        print(test_acc, test_logloss, mod[1])\n",
    "        if test_logloss <= top_logloss:\n",
    "            best_mod = mod\n",
    "            top_logloss = test_logloss\n",
    "    \n",
    "    return best_mod\n",
    "\n",
    "\n",
    "def plot_xgb(mod):\n",
    "    '''\n",
    "    Plots the validation error at each epoch during the training of an xgboost model\n",
    "    '''\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    eval_res = mod.evals_result()\n",
    "    ax[0].plot(range(0, len(eval_res['validation_0']['mlogloss'])),\n",
    "               eval_res['validation_0']['mlogloss'])\n",
    "    ax[1].plot(range(0, len(eval_res['validation_0']['merror'])),\n",
    "               eval_res['validation_0']['merror'])\n",
    "    ax[0].set(xlabel = 'Training Rounds', ylabel = 'Log Loss')\n",
    "    ax[1].set(xlabel='Training Rounds', ylabel='Classification Error')\n",
    "    ax[0].set_title('Log Loss on Validation Set')\n",
    "    ax[1].set_title('Classification Error on Validation Set')\n",
    "    plt.tight_layout()\n",
    "    plt.show()   \n",
    "\n",
    "\n",
    "param_grids = {\n",
    "                'xgb': \n",
    "                {'subsample': [.95],\n",
    "                'colsample_bytree' : [1],\n",
    "                'gamma': [5],\n",
    "                'lambda': [0],\n",
    "                'max_delta_step':[1],\n",
    "                'eta': [.1], 'n_estimators': [500],\n",
    "                'min_child_weight': [9], 'max_depth': [9],\n",
    "                'objective': ['multi:softmax'],\n",
    "                'num_class': [3]\n",
    "               },\n",
    "               'svc':\n",
    "               {'C': [1], 'gamma': [1e-3], 'kernel':['rbf'], 'probability':[1]\n",
    "               },\n",
    "               'softmax': \n",
    "               {'C': np.geomspace(1e-3, 1e3, 100),\n",
    "                'multi_class': ['multinomial'], 'penalty':['l1'],\n",
    "               'solver': ['saga']} \n",
    "            }\n",
    "mods = [('xgb', XGBClassifier(use_label_encoder=False)), \n",
    "        ('svc', SVC()), \n",
    "        ('softmax', LogisticRegression())\n",
    "       ]\n",
    "\n",
    "best_mod_sklearn = model_select_sklearn(param_grids, mods, X_tr, y_tr, X_val, y_val)\n",
    "best_mod_sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also built a neural network using Pytorch but its performance ended up being only marginally better than the others so we still prefer the simpler multinomial logit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 1.0/12.3 GiB<br>Using AsyncHyperBand: num_stopped=49\n",
       "Bracket: Iter 64.000: None | Iter 16.000: -0.9983652085065842 | Iter 4.000: -1.0000128746032715 | Iter 1.000: -1.0152829587459564<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/7.13 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /home/swayam/ray_results/DEFAULT_2022-08-15_18-44-57<br>Number of trials: 50/50 (50 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status    </th><th>loc  </th><th>activation             </th><th style=\"text-align: right;\">  batch</th><th style=\"text-align: right;\">  depth</th><th style=\"text-align: right;\">  epochs</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">     prob</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">  width</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  training_iteration</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DEFAULT_e31dd_00000</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      25</td><td style=\"text-align: right;\">0.000338922</td><td style=\"text-align: right;\">0.1614   </td><td style=\"text-align: right;\">   0.000869629</td><td style=\"text-align: right;\">     13</td><td style=\"text-align: right;\">1.00026 </td><td style=\"text-align: right;\">0.530303</td><td style=\"text-align: right;\">                  25</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00001</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.000142961</td><td style=\"text-align: right;\">0.243938 </td><td style=\"text-align: right;\">   0.000268192</td><td style=\"text-align: right;\">      7</td><td style=\"text-align: right;\">1.05853 </td><td style=\"text-align: right;\">0.439394</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00002</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">     150</td><td style=\"text-align: right;\">0.000286322</td><td style=\"text-align: right;\">0.224242 </td><td style=\"text-align: right;\">   0.000925807</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">1.10525 </td><td style=\"text-align: right;\">0.293939</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00003</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.000149599</td><td style=\"text-align: right;\">0.163603 </td><td style=\"text-align: right;\">   0.000680447</td><td style=\"text-align: right;\">      9</td><td style=\"text-align: right;\">1.07855 </td><td style=\"text-align: right;\">0.424242</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00004</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.000208292</td><td style=\"text-align: right;\">0.203616 </td><td style=\"text-align: right;\">   0.000596882</td><td style=\"text-align: right;\">     11</td><td style=\"text-align: right;\">1.07014 </td><td style=\"text-align: right;\">0.424242</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00005</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      25</td><td style=\"text-align: right;\">0.00143682 </td><td style=\"text-align: right;\">0.203124 </td><td style=\"text-align: right;\">   0.000939835</td><td style=\"text-align: right;\">     13</td><td style=\"text-align: right;\">0.998161</td><td style=\"text-align: right;\">0.524242</td><td style=\"text-align: right;\">                  16</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00006</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.000726084</td><td style=\"text-align: right;\">0.180434 </td><td style=\"text-align: right;\">   0.000333947</td><td style=\"text-align: right;\">     14</td><td style=\"text-align: right;\">1.00744 </td><td style=\"text-align: right;\">0.521212</td><td style=\"text-align: right;\">                   4</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00007</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.00110958 </td><td style=\"text-align: right;\">0.185578 </td><td style=\"text-align: right;\">   5.27318e-05</td><td style=\"text-align: right;\">      7</td><td style=\"text-align: right;\">1.04863 </td><td style=\"text-align: right;\">0.466667</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00008</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">     150</td><td style=\"text-align: right;\">0.00400313 </td><td style=\"text-align: right;\">0.107893 </td><td style=\"text-align: right;\">   0.00098518 </td><td style=\"text-align: right;\">     12</td><td style=\"text-align: right;\">1.01768 </td><td style=\"text-align: right;\">0.481818</td><td style=\"text-align: right;\">                   4</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00009</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      75</td><td style=\"text-align: right;\">0.000454555</td><td style=\"text-align: right;\">0.21837  </td><td style=\"text-align: right;\">   0.00049765 </td><td style=\"text-align: right;\">     12</td><td style=\"text-align: right;\">1.03229 </td><td style=\"text-align: right;\">0.521212</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00010</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.000142347</td><td style=\"text-align: right;\">0.0977633</td><td style=\"text-align: right;\">   0.000569041</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">1.15918 </td><td style=\"text-align: right;\">0.254545</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00011</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">     150</td><td style=\"text-align: right;\">0.000541198</td><td style=\"text-align: right;\">0.0915172</td><td style=\"text-align: right;\">   0.000547139</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">1.06095 </td><td style=\"text-align: right;\">0.466667</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00012</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">      25</td><td style=\"text-align: right;\">0.000483434</td><td style=\"text-align: right;\">0.0767302</td><td style=\"text-align: right;\">   0.000283123</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">1.00246 </td><td style=\"text-align: right;\">0.521212</td><td style=\"text-align: right;\">                   4</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00013</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">0.000463078</td><td style=\"text-align: right;\">0.216875 </td><td style=\"text-align: right;\">   0.000354347</td><td style=\"text-align: right;\">      9</td><td style=\"text-align: right;\">1.07474 </td><td style=\"text-align: right;\">0.424242</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00014</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">     150</td><td style=\"text-align: right;\">0.00138814 </td><td style=\"text-align: right;\">0.249186 </td><td style=\"text-align: right;\">   0.000540078</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">1.05841 </td><td style=\"text-align: right;\">0.457576</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00015</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      25</td><td style=\"text-align: right;\">0.010002   </td><td style=\"text-align: right;\">0.110897 </td><td style=\"text-align: right;\">   0.000882935</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">1.0342  </td><td style=\"text-align: right;\">0.436364</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00016</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.0173064  </td><td style=\"text-align: right;\">0.227197 </td><td style=\"text-align: right;\">   0.000678518</td><td style=\"text-align: right;\">      9</td><td style=\"text-align: right;\">1.06004 </td><td style=\"text-align: right;\">0.424242</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00017</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      75</td><td style=\"text-align: right;\">0.0101977  </td><td style=\"text-align: right;\">0.15281  </td><td style=\"text-align: right;\">   0.000298653</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">1.04013 </td><td style=\"text-align: right;\">0.439394</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00018</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.000102015</td><td style=\"text-align: right;\">0.179177 </td><td style=\"text-align: right;\">   1.70099e-05</td><td style=\"text-align: right;\">     11</td><td style=\"text-align: right;\">1.03518 </td><td style=\"text-align: right;\">0.478788</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00019</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">0.00322609 </td><td style=\"text-align: right;\">0.156876 </td><td style=\"text-align: right;\">   0.000269287</td><td style=\"text-align: right;\">      7</td><td style=\"text-align: right;\">1.00621 </td><td style=\"text-align: right;\">0.521212</td><td style=\"text-align: right;\">                   4</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00020</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.00037111 </td><td style=\"text-align: right;\">0.154724 </td><td style=\"text-align: right;\">   0.000438658</td><td style=\"text-align: right;\">     10</td><td style=\"text-align: right;\">1.06754 </td><td style=\"text-align: right;\">0.521212</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00021</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">      75</td><td style=\"text-align: right;\">0.0779018  </td><td style=\"text-align: right;\">0.238279 </td><td style=\"text-align: right;\">   0.000530788</td><td style=\"text-align: right;\">     11</td><td style=\"text-align: right;\">1.09459 </td><td style=\"text-align: right;\">0.424242</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00022</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      75</td><td style=\"text-align: right;\">0.0616067  </td><td style=\"text-align: right;\">0.0792992</td><td style=\"text-align: right;\">   0.000698336</td><td style=\"text-align: right;\">     13</td><td style=\"text-align: right;\">1.08065 </td><td style=\"text-align: right;\">0.424242</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00023</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.000275191</td><td style=\"text-align: right;\">0.231567 </td><td style=\"text-align: right;\">   0.000946742</td><td style=\"text-align: right;\">     11</td><td style=\"text-align: right;\">1.07711 </td><td style=\"text-align: right;\">0.424242</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00024</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      25</td><td style=\"text-align: right;\">0.00324976 </td><td style=\"text-align: right;\">0.0844608</td><td style=\"text-align: right;\">   0.000531028</td><td style=\"text-align: right;\">     14</td><td style=\"text-align: right;\">1.01309 </td><td style=\"text-align: right;\">0.518182</td><td style=\"text-align: right;\">                  16</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00025</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">0.000848177</td><td style=\"text-align: right;\">0.0958116</td><td style=\"text-align: right;\">   0.000541015</td><td style=\"text-align: right;\">     13</td><td style=\"text-align: right;\">0.998979</td><td style=\"text-align: right;\">0.530303</td><td style=\"text-align: right;\">                  16</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00026</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.000449669</td><td style=\"text-align: right;\">0.150559 </td><td style=\"text-align: right;\">   0.000584204</td><td style=\"text-align: right;\">      7</td><td style=\"text-align: right;\">1.04982 </td><td style=\"text-align: right;\">0.439394</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00027</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.000461132</td><td style=\"text-align: right;\">0.210495 </td><td style=\"text-align: right;\">   0.000685369</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">1.15097 </td><td style=\"text-align: right;\">0.321212</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00028</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.028424   </td><td style=\"text-align: right;\">0.105571 </td><td style=\"text-align: right;\">   0.000132039</td><td style=\"text-align: right;\">     11</td><td style=\"text-align: right;\">1.09166 </td><td style=\"text-align: right;\">0.424242</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00029</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.000147853</td><td style=\"text-align: right;\">0.247277 </td><td style=\"text-align: right;\">   0.000832935</td><td style=\"text-align: right;\">     14</td><td style=\"text-align: right;\">1.09931 </td><td style=\"text-align: right;\">0.424242</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00030</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      25</td><td style=\"text-align: right;\">0.00411478 </td><td style=\"text-align: right;\">0.171616 </td><td style=\"text-align: right;\">   0.000706885</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">1.0142  </td><td style=\"text-align: right;\">0.533333</td><td style=\"text-align: right;\">                   4</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00031</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.0069445  </td><td style=\"text-align: right;\">0.170869 </td><td style=\"text-align: right;\">   0.000344052</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">1.01118 </td><td style=\"text-align: right;\">0.527273</td><td style=\"text-align: right;\">                   4</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00032</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">0.0166396  </td><td style=\"text-align: right;\">0.22384  </td><td style=\"text-align: right;\">   0.000525971</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">1.05849 </td><td style=\"text-align: right;\">0.442424</td><td style=\"text-align: right;\">                   4</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00033</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.0378705  </td><td style=\"text-align: right;\">0.188685 </td><td style=\"text-align: right;\">   0.000731034</td><td style=\"text-align: right;\">     14</td><td style=\"text-align: right;\">1.02038 </td><td style=\"text-align: right;\">0.524242</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00034</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">      25</td><td style=\"text-align: right;\">0.0647594  </td><td style=\"text-align: right;\">0.0912131</td><td style=\"text-align: right;\">   0.000629784</td><td style=\"text-align: right;\">     10</td><td style=\"text-align: right;\">1.1081  </td><td style=\"text-align: right;\">0.424242</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00035</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.000356579</td><td style=\"text-align: right;\">0.201054 </td><td style=\"text-align: right;\">   0.000627354</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">1.08949 </td><td style=\"text-align: right;\">0.451515</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00036</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">     150</td><td style=\"text-align: right;\">0.00035967 </td><td style=\"text-align: right;\">0.137129 </td><td style=\"text-align: right;\">   0.000247666</td><td style=\"text-align: right;\">     14</td><td style=\"text-align: right;\">1.0263  </td><td style=\"text-align: right;\">0.530303</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00037</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      25</td><td style=\"text-align: right;\">0.00128402 </td><td style=\"text-align: right;\">0.194199 </td><td style=\"text-align: right;\">   0.000150319</td><td style=\"text-align: right;\">     13</td><td style=\"text-align: right;\">0.999693</td><td style=\"text-align: right;\">0.527273</td><td style=\"text-align: right;\">                  16</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00038</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.00296418 </td><td style=\"text-align: right;\">0.159326 </td><td style=\"text-align: right;\">   0.000142613</td><td style=\"text-align: right;\">     14</td><td style=\"text-align: right;\">1.01639 </td><td style=\"text-align: right;\">0.509091</td><td style=\"text-align: right;\">                  16</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00039</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.000114361</td><td style=\"text-align: right;\">0.163488 </td><td style=\"text-align: right;\">   0.000516776</td><td style=\"text-align: right;\">      9</td><td style=\"text-align: right;\">1.08181 </td><td style=\"text-align: right;\">0.439394</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00040</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.000121295</td><td style=\"text-align: right;\">0.225645 </td><td style=\"text-align: right;\">   0.000790116</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">1.13441 </td><td style=\"text-align: right;\">0.254545</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00041</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">     150</td><td style=\"text-align: right;\">0.0127691  </td><td style=\"text-align: right;\">0.0776123</td><td style=\"text-align: right;\">   0.000894903</td><td style=\"text-align: right;\">     10</td><td style=\"text-align: right;\">1.00124 </td><td style=\"text-align: right;\">0.536364</td><td style=\"text-align: right;\">                   4</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00042</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.00157183 </td><td style=\"text-align: right;\">0.0941188</td><td style=\"text-align: right;\">   0.000521913</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">1.00042 </td><td style=\"text-align: right;\">0.518182</td><td style=\"text-align: right;\">                   4</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00043</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.0109674  </td><td style=\"text-align: right;\">0.244042 </td><td style=\"text-align: right;\">   0.000960327</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">1.0366  </td><td style=\"text-align: right;\">0.424242</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00044</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      25</td><td style=\"text-align: right;\">0.000335999</td><td style=\"text-align: right;\">0.247319 </td><td style=\"text-align: right;\">   0.000332259</td><td style=\"text-align: right;\">      7</td><td style=\"text-align: right;\">1.08676 </td><td style=\"text-align: right;\">0.424242</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00045</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      75</td><td style=\"text-align: right;\">0.00206636 </td><td style=\"text-align: right;\">0.190513 </td><td style=\"text-align: right;\">   0.00089956 </td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">1.00393 </td><td style=\"text-align: right;\">0.539394</td><td style=\"text-align: right;\">                   4</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00046</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">0.0267881  </td><td style=\"text-align: right;\">0.167722 </td><td style=\"text-align: right;\">   5.38482e-05</td><td style=\"text-align: right;\">     14</td><td style=\"text-align: right;\">1.07019 </td><td style=\"text-align: right;\">0.424242</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00047</td><td>TERMINATED</td><td>     </td><td>PReLU(num_parameters=1)</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      75</td><td style=\"text-align: right;\">0.000538272</td><td style=\"text-align: right;\">0.188545 </td><td style=\"text-align: right;\">   0.000391525</td><td style=\"text-align: right;\">      9</td><td style=\"text-align: right;\">1.07126 </td><td style=\"text-align: right;\">0.424242</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00048</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      40</td><td style=\"text-align: right;\">0.0235719  </td><td style=\"text-align: right;\">0.248869 </td><td style=\"text-align: right;\">   0.0008857  </td><td style=\"text-align: right;\">     11</td><td style=\"text-align: right;\">1.0278  </td><td style=\"text-align: right;\">0.442424</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>DEFAULT_e31dd_00049</td><td>TERMINATED</td><td>     </td><td>ReLU()                 </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.000651599</td><td style=\"text-align: right;\">0.102896 </td><td style=\"text-align: right;\">   0.000171991</td><td style=\"text-align: right;\">     11</td><td style=\"text-align: right;\">1.01646 </td><td style=\"text-align: right;\">0.527273</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 18:47:24,999\tINFO tune.py:450 -- Total run time: 150.92 seconds (147.72 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'depth': 1, 'width': 13, 'activation': ReLU(), 'epochs': 25, 'batch': 4, 'lr': 0.0014368170918313472, 'prob': 0.20312413281679786, 'weight_decay': 0.000939835435266903}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5606)\n",
      "0.938477098941803\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    '''\n",
    "    Defines a feedforward neural network where each layer is a compostion of an\n",
    "    affine function with a nonlinear activation function that is applied elementwise.\n",
    "    Dropout is applied to each layer except the output layer for regularization purposes.\n",
    "    \n",
    "    Attributes:\n",
    "    linears: the linear layers\n",
    "    output: the ouput layer\n",
    "    activation: the activation function\n",
    "    dropout: applies dropout\n",
    "    '''\n",
    "    def __init__(self, depth, width, activation, prob, num_classes):\n",
    "        '''\n",
    "        Constructor\n",
    "        \n",
    "        Parameters:\n",
    "        depth (int): the number of hidden layers\n",
    "        width (int): the number of units in each layer\n",
    "        activation (torch.nn): the activation function (same one used for each layer)\n",
    "        prob (float): the probability with which a neuron has its outgoing \n",
    "        connections killed in a dropout layer\n",
    "        num_classes (int): the number of classes in our classification problem\n",
    "            \n",
    "        '''\n",
    "        super(Net, self).__init__()\n",
    "        self.linears = nn.ModuleList(\n",
    "            [nn.Linear(width, width) if i!= 0 else nn.Linear(\n",
    "            NUM_FEATS, width) for i in range(depth)])\n",
    "        self.output = nn.Linear(width, num_classes)\n",
    "        self.activation = activation\n",
    "        #self.softmax = nn.Softmax()\n",
    "        self.dropout = nn.Dropout(prob) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        for l in self.linears:\n",
    "            x = self.dropout(self.activation(l(x)))\n",
    "        x = self.output(x)\n",
    "        #x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MatchData(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(MatchData, self).__init__()\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.int64)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index,], self.y[index]\n",
    "\n",
    "\n",
    "def normalize(X, mu, sd):\n",
    "    '''\n",
    "    Normalizes X by subtracting MU and dividing by SD\n",
    "    '''\n",
    "    return (X - mu) / sd\n",
    "\n",
    "\n",
    "def compute_acc(labels, outputs):\n",
    "    '''\n",
    "    Converts raw ouput from network to a class prediction and computes accuracy\n",
    "    relative to true labels\n",
    "    '''\n",
    "    preds = torch.argmax(outputs.data, 1)\n",
    "    \n",
    "    return (preds == labels).sum() / labels.size(0)\n",
    "\n",
    "\n",
    "def train_nn(param_grid, criterion=None, X_tr=None, y_tr=None, X_val=None,\n",
    "             y_val=None, checkpoint_dir=None):\n",
    "    '''\n",
    "    Trains neural net with a particular configuration of hyperparameters given by \n",
    "    the param grid. At each epoch the validation loss and accuracy is reported to ray tune.\n",
    "    \n",
    "    Parameters:\n",
    "    param_grid (dic): maps hyperparameters to the particular value being tested\n",
    "    Xtr (np.array): training data\n",
    "    ytr (np.array): 1D array of training labels\n",
    "    Xval (np.array): validation data\n",
    "    yval (np.array): 1D array of validation labels\n",
    "    '''\n",
    "    net = Net(param_grid['depth'], param_grid['width'], param_grid['activation'],\n",
    "              param_grid['prob'], NUM_CLASSES)\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    if param_grid['activation'] == 'nn.PReLU()':\n",
    "        optimizer = optim.Adam([\n",
    "            {'params': net.activation.parameters(), 'weight_decay':0},\n",
    "            {'params': net.linears.parameters()},\n",
    "            {'params': net.output.parameters()}\n",
    "        ],\n",
    "            lr=param_grid['lr'], weight_decay=param_grid['weight_decay'])\n",
    "    else:\n",
    "        optimizer = optim.Adam(net.parameters(), lr=param_grid['lr'],\n",
    "                         weight_decay= param_grid['weight_decay'])\n",
    "    if checkpoint_dir:\n",
    "        mod_state, optim_state = torch.load(os.path.join(checkpoint_dir,\n",
    "                                                         'checkpoint'))\n",
    "        net.load_state_dict(mod_state)\n",
    "        optimizer.load_state_dict(optim_state)\n",
    "    trainset = MatchData(X_tr, y_tr)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=param_grid['batch'], \n",
    "                                              shuffle=True)\n",
    "    \n",
    "    for epoch in range(param_grid['epochs']):\n",
    "        train_loop(trainloader, optimizer, criterion, net)\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            val_labels = torch.tensor(y_val.values, dtype=torch.int64)\n",
    "            val_outputs = net(torch.tensor(X_val.values, dtype=torch.float32))\n",
    "            val_loss = criterion(val_outputs, val_labels).item()\n",
    "            val_acc = compute_acc(val_labels, val_outputs)\n",
    "            net.train()\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, 'checkpoint')\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "            \n",
    "        tune.report(loss=val_loss, acc=val_acc)\n",
    "\n",
    "def train_loop(trainloader, optimizer, criterion, net):\n",
    "    '''\n",
    "    Loops over training data and updates the weights according the optimzation algorithm\n",
    "    \n",
    "    Parameters:\n",
    "    trainloader: allows for iterating over the training set in batches\n",
    "    optimizer: implements optimzation algorithm used, controls weight update\n",
    "    criterion: defines the loss function that is being optimized\n",
    "    net: the model \n",
    "    '''\n",
    "    \n",
    "    running_loss = 0\n",
    "    for idx, batch in enumerate(trainloader):\n",
    "        inputs, labels = batch\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        #print(epoch, running_loss)\n",
    "    mean_loss = running_loss / len(trainloader.dataset)\n",
    "    #print(f\"Train Loss: {mean_loss}\")\n",
    "\n",
    "\n",
    "        \n",
    "def model_select_nn(param_grid, criterion, scheduler, num_trials, X_tr, y_tr,\n",
    "                    X_val, y_val, X_tst, y_tst):\n",
    "    '''\n",
    "    Uses tune to draw samples of hyperparam configurations from the search space\n",
    "    defined by the parameter grid and fits a model for each sample. The model with\n",
    "    the lowest validation loss is chosen and is retrained on both the training and \n",
    "    validation set. The test accuracy is reported.\n",
    "    \n",
    "    Parameters:\n",
    "    param_grid (dic): maps hyperparam to a distribution to draw samples from\n",
    "    scheduler: algorithm used to implement early stopping and peturb hyperparams\n",
    "    num_trials (int): the number of samples to draw\n",
    "    Xtr (np.array): training data\n",
    "    ytr (np.array): 1D array of training labels\n",
    "    Xval (np.array): validation data\n",
    "    yval (np.array): 1D array of validation labels\n",
    "    Xtst (np.array): test data\n",
    "    ytst (np.array): 1D array of test labels\n",
    "    '''\n",
    "    reporter = tune.JupyterNotebookReporter(\n",
    "        metric_columns = ['loss', 'acc', 'training_iteration'], overwrite=1)\n",
    "    res = tune.run(partial(\n",
    "        train_nn, criterion=criterion, X_tr=X_tr, y_tr=y_tr, X_val=X_val, y_val=y_val),\n",
    "             resources_per_trial={'cpu':4, 'gpu': 0},\n",
    "             config=param_grid,\n",
    "             scheduler=scheduler,\n",
    "             progress_reporter=reporter,\n",
    "             num_samples=num_trials)\n",
    "    best_trial = res.get_best_trial('loss', 'min')\n",
    "    print(best_trial.config)\n",
    "    best_mod = Net(\n",
    "        best_trial.config['depth'], best_trial.config['width'], best_trial.config['activation'],\n",
    "                  best_trial.config['prob'], NUM_CLASSES)\n",
    "    #best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    #mod_state, optim_state = torch.load(os.path.join(best_checkpoint_dir, 'checkpoint'))\n",
    "    #best_mod.load_state_dict(mod_state)\n",
    "    train_val_set = MatchData(pd.concat((X_tr, X_val)), pd.concat((y_tr, y_val)))\n",
    "    train_val_loader = torch.utils.data.DataLoader(\n",
    "        train_val_set, batch_size=best_trial.config['batch'],\n",
    "                                                  shuffle=True)\n",
    "    \n",
    "    if best_trial.config['activation'] == 'nn.PReLU()':\n",
    "        optimizer = optim.Adam([\n",
    "            {'params': best_mod.activation.parameters(), 'weight_decay':0},\n",
    "            {'params': best_mod.linears.parameters()},\n",
    "            {'params': best_mod.output.parameters()}\n",
    "        ],\n",
    "            lr=best_trial.config['lr'], weight_decay=best_trial.config['weight_decay'])\n",
    "    else:\n",
    "        optimizer = optim.Adam(best_mod.parameters(), lr=best_trial.config['lr'],\n",
    "                         weight_decay=best_trial.config['weight_decay'])\n",
    "    \n",
    "    for epoch in range(best_trial.config['epochs']):\n",
    "        train_loop(train_val_loader, optimizer, criterion, best_mod)  \n",
    "    best_mod.eval()\n",
    "    with torch.no_grad():\n",
    "        test_labels = torch.tensor(y_tst.values, dtype=torch.int64)\n",
    "        test_outputs = best_mod(torch.tensor(X_tst.values, dtype=torch.float32))\n",
    "        print(compute_acc(test_labels, test_outputs))\n",
    "        print(criterion(test_outputs, test_labels).item())\n",
    "    \n",
    "    return best_mod\n",
    "\n",
    "\n",
    "MU = np.mean(X_tr)\n",
    "SD = np.sqrt(np.diag(np.cov(X_tr.T)))\n",
    "X_tr = normalize(X_tr, MU, SD)\n",
    "X_val = normalize(X_val, MU, SD)\n",
    "X_tst = normalize(X_tst, MU, SD)\n",
    "\n",
    "param_grid = {\n",
    "    'depth': tune.randint(1, 5),\n",
    "    'width': tune.randint(3, 15),\n",
    "    'activation': tune.choice([nn.ReLU(), nn.PReLU()]),\n",
    "    'epochs': tune.choice([10, 25, 40, 50, 75, 150]),\n",
    "    'batch': tune.choice([2**i for i in range(1, 4)]),\n",
    "    'lr': tune.loguniform(1e-4, 1e-1),\n",
    "    #'momentum': tune.uniform(.9, .99),\n",
    "    'prob': tune.uniform(.075,.25),\n",
    "    'weight_decay':tune.uniform(1e-6, 1e-3)\n",
    "}\n",
    "\n",
    "'''\n",
    "param_grid = {\n",
    "    'depth': tune.randint(5,7),\n",
    "    'width': tune.randint(8, 11),\n",
    "    'activation': tune.choice([nn.ReLU(), nn.PReLU()]),\n",
    "    'epochs': tune.choice([30, 40, 50, 75, 100 ,150]),\n",
    "    'batch': tune.choice([2**i for i in range(1,3)]),\n",
    "    'lr': tune.loguniform(1e-4, 1e-2),\n",
    "    #'momentum': tune.uniform(.95, .99),\n",
    "    'prob': tune.uniform(.05, .25),\n",
    "    'weight_decay': tune.uniform(1e-5, 1e-3)\n",
    "}\n",
    "'''\n",
    "scheduler = tune.schedulers.ASHAScheduler(metric = 'loss', mode='min')\n",
    "num_trials = 50\n",
    "loss = nn.CrossEntropyLoss()\n",
    "best_mod_nn = model_select_nn(param_grid, loss, scheduler, num_trials, X_tr,\n",
    "                              y_tr, X_val, y_val, X_tst, y_tst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gather all the information relevant to betting into one DataFrame: the true label, the predicted label, the odds for the 3 outcomes and the probability outputted by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>predicted</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.466520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>23.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.755019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>1.61</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.50</td>\n",
       "      <td>0.658482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>1.20</td>\n",
       "      <td>7.5</td>\n",
       "      <td>17.00</td>\n",
       "      <td>0.734444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1658</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>1.44</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.551273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>19.00</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.707251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.473823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>1.90</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.393682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.380261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>1.30</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>0.578151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     target predicted  B365H  B365D  B365A  probability\n",
       "1650      A         A   5.00    4.0   1.75     0.466520\n",
       "1652      A         A  23.00    8.0   1.16     0.755019\n",
       "1655      H         H   1.61    4.0   6.50     0.658482\n",
       "1656      H         H   1.20    7.5  17.00     0.734444\n",
       "1658      H         H   1.44    5.0   7.50     0.551273\n",
       "...     ...       ...    ...    ...    ...          ...\n",
       "1970      A         A  19.00    8.5   1.16     0.707251\n",
       "1971      A         A   3.25    3.8   2.20     0.473823\n",
       "1972      H         H   1.90    4.2   3.80     0.393682\n",
       "1973      A         A   2.50    3.6   2.90     0.380261\n",
       "1975      H         H   1.30    6.0  11.00     0.578151\n",
       "\n",
       "[186 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_betting_df(datasets, model, y_tst, indices):\n",
    "    '''\n",
    "    Creates DataFrame containing the true outcome, the predicted outcome,\n",
    "    the probability associted with the predicted outcome and betting information\n",
    "    for each match in the test set.\n",
    "    \n",
    "    Parameters:\n",
    "    datasets (list): a list of DataFrames each containing the raw data for a single season\n",
    "    model (sklearn obj): the best model\n",
    "    y_tst (np.array): a 1D array of targets\n",
    "    indices (list): list of indices into the original DataFrames that make up datasets\n",
    "    \n",
    "    Returns: \n",
    "    bets_df (DataFrame): DataFrame as described above\n",
    "    '''\n",
    "    odds_df = pd.DataFrame()\n",
    "    for i, data in enumerate(datasets):\n",
    "        odds_df = pd.concat((\n",
    "            odds_df, data.iloc[:,BET_COL_IDX:BET_COL_IDX+3].iloc[indices[i]]))\n",
    "    odds_df.reset_index(drop=True, inplace=True)\n",
    "    ypred = model.predict(X_tst)\n",
    "    probs = pd.Series(np.max(model.predict_proba(X_tst), axis=1), index=y_tst.index)\n",
    "    ys = pd.DataFrame({'target': y_tst, 'predicted': ypred})\n",
    "    ys_np = np.where(ys==2,'H', np.where(ys==0, 'A', 'D'))\n",
    "    ys = pd.DataFrame(ys_np, columns=ys.columns, index=ys.index)\n",
    "    bets_df = ys.join(odds_df)\n",
    "    bets_df['probability'] = probs\n",
    "    return bets_df\n",
    "\n",
    "bet_df = create_betting_df(datasets, best_mod_sklearn, y_tst, indices)\n",
    "bet_df.loc[bet_df['target'] == bet_df['predicted']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a few betting strategies, all of which are quite straightforward. The most viable one is the conservative strategy which consists in placing a bet proportional to the probability outputted by the model, only if the probability is greater than some threshold; so we only placed bets if we met some confidence level. We seek the largest return subject to a minimum number of bets placed, as the returns on strategies that place a low number of bets are subject to high variance.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_strat(row, factor=1, prop_to_prob=False):\n",
    "    '''\n",
    "    Defines a simple betting strategy where the stake is equal to a constant times\n",
    "    the probability of the class outputted by the model, or it is simply the constant.\n",
    "    \n",
    "    Parameters: \n",
    "    row (Series): contains prediction, true result and betting information\n",
    "    factor (int): the stake\n",
    "    prop_to_prob (bool): whether the stake should be proportional to the probability\n",
    "    \n",
    "    Returns: -stake if bet is lost, and the stake multiplied by the odds if won   \n",
    "    '''\n",
    "    \n",
    "    if prop_to_prob:\n",
    "        stake = factor * row[5]\n",
    "    else:\n",
    "        stake = factor\n",
    "    if row[0] != row[1]:\n",
    "        return -stake \n",
    "    if row[0] == 'H':\n",
    "        return  stake * (row[2])\n",
    "    elif row[0] == 'A':\n",
    "        return  stake * (row[4])\n",
    "    else:\n",
    "        return stake * (row[3])\n",
    "\n",
    "def decimal_to_prob(decimal):\n",
    "    '''\n",
    "    Obtains the implied probability given decimal odds.\n",
    "    '''\n",
    "    return 1/decimal\n",
    "    \n",
    "def value_strat(row, factor, prop_to_prob):\n",
    "    '''\n",
    "    Defines a betting strategy that identifies good value bets by comparing the\n",
    "    model's confidence of the result to the bookmakers'confidence.\n",
    "    \n",
    "    Parameters: \n",
    "    row (Series): contains prediction, true result and betting information\n",
    "    factor (int): the stake\n",
    "    prop_to_prob (bool): whether the stake should be proportional to the probability\n",
    "    \n",
    "    Returns: -stake if bet is lost, and the stake multiplied by the odds if won \n",
    "    \n",
    "    '''\n",
    "    if row[5] >= decimal_to_prob(min(row[2:5])):\n",
    "        return simple_strat(row, factor, prop_to_prob)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def conservative_strat(row, factor, prop_to_prob, threshold):\n",
    "    '''\n",
    "    Defines a conservative betting strategy that only places a bet only if the\n",
    "    model is confident in its prediction. In other words, we place a bet if the\n",
    "    probability is greater than some threshold.\n",
    "    \n",
    "    Parameters: \n",
    "    row (Series): contains prediction, true result and betting information\n",
    "    factor (int): the stake\n",
    "    prop_to_prob (bool): whether the stake should be proportional to the probability\n",
    "    threshold (float): controls the conservativeness of the strategy, should lie in [0,1] \n",
    "    \n",
    "    Returns: -stake if bet is lost, and the stake multiplied by the odds if won \n",
    "    \n",
    "    '''\n",
    "    if row[5] >= threshold:\n",
    "        return simple_strat(row, factor, prop_to_prob)\n",
    "    else: \n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conservative_strat\n",
      "Number of bets placed: 34\n",
      "Total winnings: 669.5649263995439\n",
      "     target predicted  B365H  B365D  B365A  probability   winnings\n",
      "1703      H         H   1.61    4.0    6.5     0.741815  29.858067\n",
      "Div                             E0\n",
      "Date                    03/11/2018\n",
      "HomeTeam                Everton FC\n",
      "AwayTeam    Brighton & Hove Albion\n",
      "FTHG                             3\n",
      "FTAG                             1\n",
      "Name: 103, dtype: object\n",
      "Expenditure: 657.1614280206487\n",
      "1.8874355447571147% return\n",
      "conservative_strat\n",
      "Number of bets placed: 23\n",
      "Total winnings: 488.386334518552\n",
      "     target predicted  B365H  B365D  B365A  probability   winnings\n",
      "1946      H         H   1.28    6.0   13.0     0.813001  26.016022\n",
      "Div                             E0\n",
      "Date                    23/04/2019\n",
      "HomeTeam         Tottenham Hotspur\n",
      "AwayTeam    Brighton & Hove Albion\n",
      "FTHG                             1\n",
      "FTAG                             0\n",
      "Name: 346, dtype: object\n",
      "Expenditure: 454.547085997157\n",
      "7.444607954566594% return\n"
     ]
    }
   ],
   "source": [
    "def evaluate_strategy(strat, bets_df, datasets, args=None):\n",
    "    '''\n",
    "    Evaluates the performance of a betting salary by noting the number of bets placed, \n",
    "    the total winnings (not including expenditure), the most profitable bet and\n",
    "    the percent return.\n",
    "    \n",
    "    Parameters:\n",
    "    strat (function): the betting strategy\n",
    "    bets_df (DataFrame): DataFrame containing the true outcome, the predicted\n",
    "    outcome, the probability associted with the predicted outcome and betting information\n",
    "    for each match in the test set\n",
    "    datasets (list): list of DataFrames, one for each season. Used to get the\n",
    "    identity of the teams competing in the match that was the most profitable bet\n",
    "    args (tuple): positional arguements to be passed to strat\n",
    "    \n",
    "    Returns: the percent return and number of bets placed                    \n",
    "    '''\n",
    "    print(strat.__name__)\n",
    "    bet_df['winnings'] = bet_df.apply(strat, axis=1, args=args)\n",
    "    num_bets = bet_df.loc[bet_df['winnings'] != 0].shape[0]\n",
    "    print(\"Number of bets placed: {}\".format(num_bets))\n",
    "    running_winnings = bet_df['winnings'].sort_index().expanding().sum()\n",
    "    print(\"Total winnings: {}\".format(running_winnings.iloc[-1]))\n",
    "    #print(\"Losing Money:\")\n",
    "    #print(running_winnings.loc[running_winnings < 0])\n",
    "    #print(bet_df.loc[bet_df['winnings']!=0])\n",
    "    print(bet_df.loc[bet_df.index == bet_df['winnings'].idxmax()])\n",
    "    print(datasets[bet_df['winnings'].idxmax() // len(y_tst)].iloc[\n",
    "        MIN_PERIODS*10 + bet_df['winnings'].idxmax() % len(y_tst),:6])\n",
    "    if strat.__name__ == 'conservative_strat' and args[1]:\n",
    "        expenditure = args[0] * bet_df.loc[\n",
    "        bet_df['probability'] >= args[2]]['probability'].sum()\n",
    "        print(\"Expenditure: {}\".format(expenditure))\n",
    "        percent_return = 100 * (running_winnings.iloc[-1] - expenditure) / expenditure\n",
    "        print('{}% return'.format(percent_return))\n",
    "    \n",
    "    return percent_return, num_bets\n",
    "\n",
    "thresh = .725\n",
    "num_bets = len(y_tst)\n",
    "percent_return = 1\n",
    "while (num_bets >= 30 and percent_return >= 0):\n",
    "    percent_return, num_bets = evaluate_strategy(\n",
    "        conservative_strat, bet_df, datasets, args=(25, True, thresh))\n",
    "    thresh += .025\n",
    "    \n",
    "#thresh\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
